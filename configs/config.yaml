model:
  vocab_size: 30000
  d_model: 512
  nhead: 8
  num_layers: 6
  dim_feedforward: 2048
  dropout: 0.1
  num_classes: 2

training:
  batch_size: 2
  learning_rate: 0.0001
  num_epochs: 10
  
data:
  train_path: "data/processed/train.txt"
  val_path: "data/processed/val.txt"
  max_length: 512
